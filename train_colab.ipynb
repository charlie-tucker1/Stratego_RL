{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratego RL Training on Google Colab\n",
    "\n",
    "This notebook trains a PPO agent to play Stratego using GPU acceleration.\n",
    "\n",
    "**Setup:**\n",
    "1. Runtime → Change runtime type → T4 GPU (or A100 if available with Colab Pro)\n",
    "2. Run cells in order\n",
    "3. Models will be saved to your Google Drive\n",
    "\n",
    "**Expected Training Time:**\n",
    "- 2M timesteps on T4 GPU: ~6-8 hours\n",
    "- 2M timesteps on A100: ~2-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Google Drive\n",
    "!mkdir -p /content/drive/MyDrive/Stratego_RL/models\n",
    "!mkdir -p /content/drive/MyDrive/Stratego_RL/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/charlie-tucker1/Stratego_RL.git\n",
    "%cd Stratego_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch can see GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Start Fresh Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update train.py to save to Google Drive\n",
    "# This ensures checkpoints persist even if runtime disconnects\n",
    "\n",
    "import os\n",
    "\n",
    "# Read the training script\n",
    "with open('train.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update paths to use Google Drive\n",
    "content = content.replace('LOG_DIR = \"./logs\"', 'LOG_DIR = \"/content/drive/MyDrive/Stratego_RL/logs\"')\n",
    "content = content.replace('MODEL_DIR = \"./models\"', 'MODEL_DIR = \"/content/drive/MyDrive/Stratego_RL/models\"')\n",
    "\n",
    "# Write back\n",
    "with open('train.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"Updated train.py to save to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Resume from Existing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your local checkpoints to Google Drive first, then run this\n",
    "# This will continue training from your 1.75M step model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from stratego_logic import StrategoEnv\n",
    "\n",
    "# Update import for the callback and mask function\n",
    "import sys\n",
    "sys.path.insert(0, '/content/Stratego_RL')\n",
    "from train import MetricsCallback, mask_fn, make_env\n",
    "\n",
    "# Configuration\n",
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/Stratego_RL/models/stratego_ppo_1750000_steps.zip\"\n",
    "TOTAL_TIMESTEPS = 2_000_000\n",
    "RESUME_FROM_STEP = 1_750_000\n",
    "REMAINING_STEPS = TOTAL_TIMESTEPS - RESUME_FROM_STEP\n",
    "\n",
    "LOG_DIR = \"/content/drive/MyDrive/Stratego_RL/logs\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/Stratego_RL/models\"\n",
    "SAVE_FREQ = 50_000\n",
    "\n",
    "print(f\"Loading model from: {CHECKPOINT_PATH}\")\n",
    "print(f\"Resuming from step: {RESUME_FROM_STEP:,}\")\n",
    "print(f\"Remaining steps: {REMAINING_STEPS:,}\")\n",
    "\n",
    "# Load the model\n",
    "model = MaskablePPO.load(\n",
    "    CHECKPOINT_PATH,\n",
    "    tensorboard_log=LOG_DIR,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "env = DummyVecEnv([make_env])\n",
    "model.set_env(env)\n",
    "\n",
    "# Setup callbacks\n",
    "metrics_callback = MetricsCallback(eval_freq=10_000, n_eval_episodes=10)\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=SAVE_FREQ,\n",
    "    save_path=MODEL_DIR,\n",
    "    name_prefix=\"stratego_ppo\"\n",
    ")\n",
    "\n",
    "print(\"\\nContinuing training...\")\n",
    "\n",
    "# Continue training\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=REMAINING_STEPS,\n",
    "        callback=[metrics_callback, checkpoint_callback],\n",
    "        progress_bar=True,\n",
    "        reset_num_timesteps=False  # Keep timestep counter\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user\")\n",
    "except ValueError as e:\n",
    "    if \"Simplex()\" in str(e):\n",
    "        print(\"\\n\\nTraining stopped due to numerical stability issue\")\n",
    "        print(\"Your checkpoints are saved!\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Save final model\n",
    "final_path = os.path.join(MODEL_DIR, \"stratego_ppo_final\")\n",
    "model.save(final_path)\n",
    "print(f\"\\nFinal model saved to: {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard in the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/Stratego_RL/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a specific checkpoint\n",
    "!python train.py --eval /content/drive/MyDrive/Stratego_RL/models/stratego_ppo_1750000_steps.zip --episodes 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoints to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all models for easy download\n",
    "!zip -r stratego_models.zip /content/drive/MyDrive/Stratego_RL/models/\n",
    "\n",
    "# Download via Colab files panel or use:\n",
    "from google.colab import files\n",
    "files.download('stratego_models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Agent Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the agent play a game with rendering\n",
    "import numpy as np\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stratego_logic import StrategoEnv\n",
    "\n",
    "# Load model\n",
    "model = MaskablePPO.load(\"/content/drive/MyDrive/Stratego_RL/models/stratego_ppo_1750000_steps.zip\")\n",
    "\n",
    "# Create environment with rendering\n",
    "env = StrategoEnv(render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Starting game...\\n\")\n",
    "\n",
    "step = 0\n",
    "while not done and step < 500:\n",
    "    action_mask = info.get(\"action_mask\", np.ones(3600))\n",
    "    action, _ = model.predict(obs, action_masks=action_mask, deterministic=True)\n",
    "    \n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: Reward = {total_reward:.2f}\")\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"\\nGame ended after {step} steps\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Winner: {env.game.winner if hasattr(env.game, 'winner') else 'Unknown'}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
